# -HW1
# 构建一个二层神经网络
## 1、数据集
MNIST 手写数字数据集是一个经典的多分类问题，通过神经网络可以很好的实现手写字体的识别。MNIST 数据集由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局的工作人员，测试集也是同样比例的手写数字数据，但保证了测试集和训练集的作者集不相交。MNIST 数据集一共有 7 万张图片，其中 6 万张是训练集，1 万张是测试集。每张图片是 28 × 28的 0-9 的手写数字图片组成。每个图片是黑底白字的形式，黑底用 0 表示，白字用 0-1 之间的浮点数表示，越接近 1，颜色越白。
## 2、设计
使用 numpy 构建一个两层神经网络分类器并进行训练，我们需要计算每层的梯度。在此任务中，采用激活函数 ReLU 进行网络构建，并通过交叉熵损失函数进行损失计算和梯度回传。
### 2.1 激活函数
神经网络通常通过矩阵乘法进行线性计算，通过激活函数引入非线性
### 2.2 梯度计算
本次使用的两层全连接层是一种线性结构，可以通过公式表示为：f(X) = W X + b，其中 W 表示神经元权重，X 表述输入数据，f(X) 表示输出值。
### 2.3 交叉熵损失函数
本任务是经典的多分类问题，我们采用多分类的交叉熵损失函数进行计算

## 3、参数查找
通过实验遍历了以上搜索空间的所有组合，最后确定在隐藏层神经元个数为 512，学习率为 0.005，L2 正则化参数为 0.005 时训练效果最优。

## 4、代码文件说明
util.py 辅助文件
multi_layer_net.py 神经网络模型文件
hyperparameter_search.py 参数查找文件
two_layers_train.py 神经网络训练文件，可视化训练和测试的loss曲线，测试的accuracy曲线
visualize_parameters.py 可视化模型参数的文件
params.pkl 保存的模型参数

## 5、如何运行
直接运行hyperparameter_search.py 进行参数查找
选择从上一步骤找到的最优参数，用于神经网络的训练，运行two_layers_train.py
上一步训练得到的模型会保存在params.pkl
运行visualize_parameters.py 可视化模型的参数
